{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "polar-billy",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "scenic-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import csv\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as tfunc\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "import torch.nn.functional as func\n",
    "import torchxrayvision as xrv\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sklearn.metrics as metrics\n",
    "import random\n",
    "import logging\n",
    "\n",
    "\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-trail",
   "metadata": {},
   "source": [
    "#  Get data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "geological-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # add data augmentations transforms here\n",
    "    transform = torchvision.transforms.Compose([xrv.datasets.XRayCenterCrop(),\n",
    "                                                xrv.datasets.XRayResizer(224)])\n",
    "    # replace the paths for the dataset here\n",
    "    d_chex_train = xrv.datasets.CheX_Dataset(imgpath='/local/nhulkund/UROP/Chexpert/data/CheXpert-v1.0-small',\n",
    "                                       csvpath=\"/local/nhulkund/UROP/Chexpert/data/CheXpert-v1.0-small/train_preprocessed.csv\",\n",
    "                                       transform=transform)\n",
    "    d_chex_valid = xrv.datasets.CheX_Dataset(imgpath='/local/nhulkund/UROP/Chexpert/data/CheXpert-v1.0-small',\n",
    "                                       csvpath=\"/local/nhulkund/UROP/Chexpert/data/CheXpert-v1.0-small/valid_preprocessed.csv\",\n",
    "                                       transform=transform)\n",
    "    return d_chex_train, d_chex_valid\n",
    "\n",
    "def get_model():\n",
    "    model = xrv.models.DenseNet(num_classes=13)\n",
    "    print(model.classifier)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "australian-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    for idx, data in enumerate(dataset):\n",
    "        data['lab']=np.nan_to_num(data['lab'],0)\n",
    "        data['lab']=np.where(data['lab']==-1, 1, data['lab']) \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-leonard",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "running-solomon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model,num_epochs,path_trained_model,train_loader,valid_loader):\n",
    "    print(\"training\")\n",
    "    # hyperparameters\n",
    "    criterion = nn.BCEWithLogitsLoss() \n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    best_valid_loss=10000\n",
    "    PATH = path_trained_model\n",
    "    \n",
    "    # going through epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # training loss\n",
    "        print(\"epoch\",epoch)\n",
    "        model.train()\n",
    "        model.to(\"cuda:0\")\n",
    "        train_loss = 0\n",
    "        count=0\n",
    "        for data_all in train_loader:\n",
    "            data=data_all['img']\n",
    "            target=data_all['lab']\n",
    "            count+=1\n",
    "            if count % 100 == 0:\n",
    "                print(\"data \", count)\n",
    "            data = data.to(\"cuda:0\")\n",
    "            target = target.to(\"cuda:0\")\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # validation loss\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data_all in valid_loader:\n",
    "                data=data_all['img']\n",
    "                target=data_all['lab']\n",
    "                data = data.to(\"cuda:0\")\n",
    "                target = target.to(\"cuda:0\")\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target) \n",
    "                valid_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        valid_loss /= len(valid_loader)\n",
    "        print(train_loss)\n",
    "        print(valid_loss)\n",
    "        \n",
    "        # saves best epoch\n",
    "        print(f'Epoch: {epoch+1}/{num_epochs}.. Training loss: {train_loss}.. Validation Loss: {valid_loss}')\n",
    "        if valid_loss < best_valid_loss:\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            best_valid_loss=valid_loss\n",
    "        print(\"Best Valid Loss so far:\", best_valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-princess",
   "metadata": {},
   "source": [
    "# Testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "continent-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(test_loader,model,data_augmentation_type):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_idx=0\n",
    "    \n",
    "    model.eval()\n",
    "    for data_all in tqdm(enumerate(test_loader)):\n",
    "        data=data_all['img']\n",
    "        target=data_all['lab']\n",
    "        batch_idx+=1\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(batch_idx)\n",
    "            \n",
    "        data = data.to(\"cuda:0\")\n",
    "        outputs = model(data)\n",
    "        value, indices = outputs.max(1)\n",
    "        index = np.array(indices.cpu())\n",
    "        predictions = torch.LongTensor(index).to(\"cuda:0\")\n",
    "        loss = F.nll_loss(outputs, predictions)\n",
    "        loss.backward()\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "    print(\"done iterating\")\n",
    "    filename=\"best_epoch_densenet121_chexpert\"+str(data_augmentation_type)\n",
    "    pd.DataFrame(result_dicts).to_csv(filename)\n",
    "    #pd.DataFrame(softmax_all).to_csv('rotations/softmax_all_puregrads'+str(angle)+\"_batch_size\"+str(batch_size))\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unique-charlotte",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting XRayResizer engine to cv2 could increase performance.\n"
     ]
    }
   ],
   "source": [
    "train,datasetValid=load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "suitable-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "trBatchSize=16\n",
    "datasetTest, datasetTrain = random_split(train, [500, len(train) - 500])  \n",
    "dataLoaderTrain = DataLoader(dataset=datasetTrain, batch_size=trBatchSize, shuffle=True,  num_workers=24, pin_memory=True)\n",
    "dataLoaderVal = DataLoader(dataset=datasetValid, batch_size=trBatchSize, shuffle=False, num_workers=24, pin_memory=True)\n",
    "dataLoaderTest = DataLoader(dataset=datasetTest, num_workers=24, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-terminology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-beaver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=1024, out_features=13, bias=True)\n",
      "training\n",
      "epoch 0\n",
      "data  100\n",
      "data  200\n",
      "data  300\n",
      "data  400\n",
      "data  500\n",
      "data  600\n",
      "data  700\n",
      "data  800\n",
      "data  900\n",
      "data  1000\n",
      "data  1100\n",
      "data  1200\n",
      "0.3902914229147166\n",
      "0.2389814406633377\n",
      "Epoch: 1/10.. Training loss: 0.3902914229147166.. Validation Loss: 0.2389814406633377\n",
      "Best Valid Loss so far: 0.2389814406633377\n",
      "epoch 1\n",
      "data  100\n",
      "data  200\n",
      "data  300\n",
      "data  400\n",
      "data  500\n",
      "data  600\n",
      "data  700\n",
      "data  800\n",
      "data  900\n",
      "data  1000\n",
      "data  1100\n",
      "data  1200\n",
      "0.3750741806324253\n",
      "0.23580903559923172\n",
      "Epoch: 2/10.. Training loss: 0.3750741806324253.. Validation Loss: 0.23580903559923172\n",
      "Best Valid Loss so far: 0.23580903559923172\n",
      "epoch 2\n",
      "data  100\n",
      "data  200\n",
      "data  300\n",
      "data  400\n",
      "data  500\n",
      "data  600\n",
      "data  700\n",
      "data  800\n",
      "data  900\n",
      "data  1000\n",
      "data  1100\n",
      "data  1200\n",
      "0.36847541970628217\n",
      "0.2330353707075119\n",
      "Epoch: 3/10.. Training loss: 0.36847541970628217.. Validation Loss: 0.2330353707075119\n",
      "Best Valid Loss so far: 0.2330353707075119\n",
      "epoch 3\n",
      "data  100\n",
      "data  200\n",
      "data  300\n",
      "data  400\n",
      "data  500\n",
      "data  600\n",
      "data  700\n",
      "data  800\n",
      "data  900\n",
      "data  1000\n",
      "data  1100\n",
      "data  1200\n",
      "0.36418312395560865\n",
      "0.23301414400339127\n",
      "Epoch: 4/10.. Training loss: 0.36418312395560865.. Validation Loss: 0.23301414400339127\n",
      "Best Valid Loss so far: 0.23301414400339127\n",
      "epoch 4\n",
      "data  100\n",
      "data  200\n",
      "data  300\n",
      "data  400\n",
      "data  500\n",
      "data  600\n",
      "data  700\n",
      "data  800\n",
      "data  900\n",
      "data  1000\n",
      "data  1100\n",
      "data  1200\n",
      "0.3611045942626186\n",
      "0.22670862823724747\n",
      "Epoch: 5/10.. Training loss: 0.3611045942626186.. Validation Loss: 0.22670862823724747\n",
      "Best Valid Loss so far: 0.22670862823724747\n",
      "epoch 5\n",
      "data  100\n",
      "data  200\n",
      "data  300\n",
      "data  400\n",
      "data  500\n",
      "data  600\n",
      "data  700\n",
      "data  800\n",
      "data  900\n",
      "data  1000\n",
      "data  1100\n",
      "data  1200\n",
      "0.35915625932591494\n",
      "0.23188655078411102\n",
      "Epoch: 6/10.. Training loss: 0.35915625932591494.. Validation Loss: 0.23188655078411102\n",
      "Best Valid Loss so far: 0.22670862823724747\n",
      "epoch 6\n",
      "data  100\n",
      "data  200\n"
     ]
    }
   ],
   "source": [
    "model=get_model()\n",
    "training(model=model,num_epochs=10,path_trained_model=\"densenet_model\",train_loader=dataLoaderTrain,valid_loader=dataLoaderVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-vancouver",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
